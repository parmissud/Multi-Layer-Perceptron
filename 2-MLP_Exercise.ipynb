{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <b>Machine Learning - SBU FALL 2024</b></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_number = ''\n",
    "Name = ''\n",
    "Last_Name = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXl9VKKjaTOr"
   },
   "source": [
    "In this notebook, you are expected to implement a fully functional MLP (Multi-Layer Perceptron) neural network from scratch. \n",
    "You are not allowed to use any libraries (including numpy). You will use the **Iris dataset** for training and testing your network, focusing on reducing the error on this dataset. \n",
    "\n",
    "\n",
    "**modify iris dataset to a version compatible with this task : binary classifiaction of if a flower is setosa (1) or not(-1)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cdv3E5_Fe_27"
   },
   "source": [
    "# Automatic Differentiation\n",
    "\n",
    "Automatic differentiation has two main methods: forward mode and reverse mode. \n",
    "PyTorch uses the reverse mode approach, and we will also use this method in this project.\n",
    "\n",
    "To learn this concept, simply click on this [link](https://auto-ed.readthedocs.io/en/latest/mod3.html#i-the-basics-of-reverse-mode) \n",
    "and read only the section \"Intuition for Example An IV\" up to the end of step six.\n",
    "\n",
    "Essentially, you need to consider a data structure to build a computational graph. \n",
    "By calling the `backward` function on the network's output, you can compute the derivative of the output \n",
    "with respect to all weights and biases of the network. (In this case, our network has only one output.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_o2mBT0MAc5T"
   },
   "source": [
    "## Visualization Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bHSUQ91rpaBc"
   },
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "import random\n",
    "import math\n",
    "from math import exp\n",
    "\n",
    "def trace(root):\n",
    "    nodes, edges = set(), set()\n",
    "\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v.children:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "\n",
    "def draw_dot(root, format='svg', rankdir='LR'):\n",
    "    \"\"\"\n",
    "    format: png | svg | ...\n",
    "    rankdir: TB (top to bottom graph) | LR (left to right)\n",
    "    \"\"\"\n",
    "    assert rankdir in ['LR', 'TB']\n",
    "    nodes, edges = trace(root)\n",
    "    dot = Digraph(format=format, graph_attr={'rankdir': rankdir})  # , node_attr={'rankdir': 'TB'})\n",
    "\n",
    "    for n in nodes:\n",
    "        dot.node(name=str(id(n)), label=\"{ %s | data %.4f | grad %.4f }\" % (n.label, n.value, n.grad), shape='record')\n",
    "        if n.operator:\n",
    "            dot.node(name=str(id(n)) + n.operator, label=n.operator)\n",
    "            dot.edge(str(id(n)) + n.operator, str(id(n)))\n",
    "\n",
    "    for n1, n2 in edges:\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2.operator)\n",
    "\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jis6y_bKE7OG"
   },
   "source": [
    "## Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ecztx1Jkili"
   },
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "\n",
    "  def __init__(self, value, label='', children=(), operator=None):\n",
    "    self.value = value\n",
    "    self.children = set(children)\n",
    "    self.operator = operator\n",
    "    self.grad = 0\n",
    "    self._backward = lambda  : None\n",
    "    self.label = label\n",
    "\n",
    "\n",
    "  def __repr__(self) -> str:\n",
    "    return f\"Tensor(label = {self.label}, value = {self.value}, grad = {self.grad}, operator = {self.operator})\"\n",
    "\n",
    "  # normal add\n",
    "  def __add__(self, other):\n",
    "\n",
    "    other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "\n",
    "    out_value = self.value + other.value\n",
    "    out = Tensor(out_value, children=(self, other), operator='+')\n",
    "\n",
    "    def backward():\n",
    "      self.grad = 1 * out.grad\n",
    "      other.grad = 1 * out.grad\n",
    "\n",
    "    out._backward = backward\n",
    "\n",
    "    return out\n",
    "\n",
    "  # reverse add\n",
    "  def __radd__(self, other):\n",
    "    return self + other\n",
    "\n",
    "  def __sub__(self, other):\n",
    "    # TODO\n",
    "\n",
    "  def __mul__(self, other):\n",
    "    # TODO\n",
    "\n",
    "  def __rmul__(self, other):\n",
    "    # TODO\n",
    "\n",
    "  def __truediv__(self, other):\n",
    "    # TODO\n",
    "\n",
    "  def __pow__(self, other):\n",
    "    # TODO\n",
    "\n",
    "  def backward(self):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGC0u8U-Nefy"
   },
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ecFayfBy_EW"
   },
   "outputs": [],
   "source": [
    "class F:\n",
    "\n",
    "  @staticmethod\n",
    "  def tanh(x: Tensor) -> Tensor:\n",
    "    # TODO\n",
    "\n",
    "    def backward():\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWebVMc6JMbc"
   },
   "source": [
    "# Neuron, Layer & MLP (Forward Section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3pbRTBAjmng1"
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "\n",
    "  def __init__(self, input_size):\n",
    "\n",
    "    self.weights = [Tensor(random.uniform(-1, 1)) for i in range(input_size)]\n",
    "    self.bias = Tensor(random.uniform(-1, 1))\n",
    "\n",
    "  def forward(self, x):\n",
    "    # TODO\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # TODO\n",
    "\n",
    "  def parameters(self):\n",
    "    # TODO\n",
    "\n",
    "  def __print__(self):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TIEV-ViUDk7j"
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "  def __init__(self, input_size, output_size):\n",
    "    self.neurons = [Neuron(input_size) for _ in range(output_size)]\n",
    "\n",
    "  def forward(self, x):\n",
    "    # TODO\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # TODO\n",
    "\n",
    "  def parameters(self):\n",
    "    # TODO\n",
    "\n",
    "  def __print__(self):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74V1J3PGeTlm"
   },
   "source": [
    "# The MLP Class Structure\n",
    "\n",
    "The `MLP` class is expected to have three main methods, which you should implement with the same structure:\n",
    "\n",
    "1. **`forward` Method:**  \n",
    "   This method performs calculations on the input and returns the output.\n",
    "\n",
    "2. **`__call__` Method:**  \n",
    "   This method simply calls the `forward` method. Essentially, we want to pass the input to the model in this way: `model(x)`.\n",
    "\n",
    "3. **`parameters` Method:**  \n",
    "   This method returns all the weights of the network in a list.\n",
    "\n",
    "---\n",
    "\n",
    "# Layers in the MLP\n",
    "\n",
    "The `MLP` class itself consists of several layers (referred to as `Layer`), which are the actual layers of the neural network. Each layer needs to know:\n",
    "- The dimensions of the input data.\n",
    "- The dimensions of the output data.\n",
    "\n",
    "In this project, all inputs are vectors. Each `Layer` consists of several neurons. For example:\n",
    "- If a layer receives a 7-dimensional vector as input (input size) and produces a 4-dimensional vector as output, then:\n",
    "  - The layer should have 4 neurons.\n",
    "  - Each neuron should have 7 weights and 1 bias.\n",
    "\n",
    "You should implement the details and structure of the layers and neurons according to the given explanation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5kB1TmeoDsPv"
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "\n",
    "  def __init__(self, input_size, layer_sizes):\n",
    "    layers_total = [input_size] + layer_sizes\n",
    "    self.layers = [Layer(layers_total[i], layers_total[i+1]) for i in range(len(layer_sizes))]\n",
    "\n",
    "  def forward(self, x):\n",
    "    # TODO\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # TODO\n",
    "\n",
    "  def parameters(self):\n",
    "    # TODO\n",
    "\n",
    "  def __print__(self):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aYt38_JdPOG"
   },
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wIeXxs1gw4q"
   },
   "source": [
    "# Optimizer\n",
    "\n",
    "Similar to the first project, the `optimizer` should have access to the network's weights. \n",
    "This time, it must update them based on their derivatives and the value of the `lr` parameter (learning rate).\n",
    "\n",
    "- **`step` Method:**  \n",
    "  This method functions similarly to the `update` method in the previous project. It updates the weights of the network.\n",
    "\n",
    "- **`grad_zero` Method:**  \n",
    "  An additional method called `grad_zero` is included, whose functionality has been explained. It is used to reset the gradients of the weights to zero after each update step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Og_hjDcedPsm"
   },
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "  def __init__(self, parameters, lr):\n",
    "    self.parameters = parameters\n",
    "    self.lr = lr\n",
    "\n",
    "  def zero_grad(self):\n",
    "    # TODO\n",
    "\n",
    "  def step(self):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vy-yOcZ2TfqP"
   },
   "source": [
    "# Training Part\n",
    "\n",
    "Prepeare the dataset in this section in bellow code snippet we place a toy example dataset just for better clarification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0]\n",
    "]\n",
    "\n",
    "Y = [1.0, -1.0, -1.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwMIvkuWOGwW"
   },
   "source": [
    "## Loss Function (SE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ux28TnpO1-aX"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def criterion(y_hats: List[Tensor], Y) -> Tensor:\n",
    "  return sum([(y_hat - y)**2 for y_hat, y in zip(y_hats, Y)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "009l3N2_OM_w"
   },
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOgU8n9bhIL_"
   },
   "source": [
    "# Training Steps for the Model\n",
    "\n",
    "1. **Calculate Model Predictions:**  \n",
    "   For each input `x`, compute the output of the model, `y_hat`.\n",
    "\n",
    "2. **Compute Error:**  \n",
    "   Calculate the error of the predictions using the Mean Squared Error (MSE) loss function for simplicity.\n",
    "\n",
    "3. **Reset Gradients of Network Variables:**  \n",
    "   Set the gradients of all variables in the network to zero. Once you implement automatic differentiation, you will understand why this step is necessary.\n",
    "\n",
    "4. **Compute Derivatives:**  \n",
    "   This is the most challenging part of the project. When this function is called, you need to calculate the derivative of the `loss` with respect to all weights and biases of the network.  \n",
    "   To implement this, you will use **Automatic Differentiation (AutoDiff)**.\n",
    "\n",
    "5. **Update Network Weights:**  \n",
    "   The `optimizer` will use the derivatives of the `loss` with respect to all weights and biases to update them in a direction that reduces the error in the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCAwwaS9Zwys"
   },
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "\n",
    "input_size = 3 # Number of features in the input layer\n",
    "layer_sizes = [2, 3, 1] # Number of neurons in each hidden and output layer\n",
    "model = MLP(input_size, layer_sizes)\n",
    "\n",
    "for _ in range(n_epochs):\n",
    "  # Forward pass: Compute predictions for the entire dataset\n",
    "  y_hats = ...\n",
    "\n",
    "  # Compute the loss\n",
    "  loss = ...\n",
    "\n",
    "  # Zero the gradients to prevent accumulation from previous iterations\n",
    "  optim.zero_grad()\n",
    "\n",
    "  # Backward pass: Compute the gradient of the loss function with respect to model parameters\n",
    "  loss.backward()\n",
    "\n",
    "  # Update the model parameters using the optimizer\n",
    "  optim.step()\n",
    "\n",
    "draw_dot(loss)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

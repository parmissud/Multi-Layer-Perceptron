# Multi-Layer Perceptron (MLP) Exercise

This repository contains a Jupyter Notebook designed to implement and train a Multi-Layer Perceptron (MLP) using PyTorch. The notebook provides hands-on exercises for understanding forward and backward propagation, optimization, and loss computation in deep learning.

## Features
- Implementation of an MLP using PyTorch.
- Understanding and applying forward and backward propagation.
- Optimization using PyTorch's `optim` module.
- Loss computation and backpropagation for training the model.

## Methods Used
- **Forward Propagation**: Computes outputs from inputs through neural network layers.
- **Backward Propagation**: Uses gradients to update model parameters.
- **MLP Architecture**: Implements a basic multi-layer perceptron for classification.
- **Optimization**: Uses gradient-based optimization techniques (`torch.optim`).
- **Loss Function**: Computes training loss for better model evaluation and improvement.

## Prerequisites
Ensure you have the following installed:
- Python 3.x
- Jupyter Notebook or Google Colab
- Required Python libraries:
  ```sh
  pip install torch numpy matplotlib
  ```

## How to Use
1. Open the Jupyter Notebook (`2-MLP_Exercise.ipynb`) in Google Colab or a local Jupyter environment.
2. Ensure PyTorch is installed and available.
3. Run the notebook cells sequentially to implement and train the MLP model.
